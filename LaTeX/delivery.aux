\relax 
\providecommand\hyper@newdestlabel[2]{}
\abx@aux@refcontext{anyt/global//global/global}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{UKenglish}{}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The structure of the source code and filesystem}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Franke function}{2}{subsection.1.2}\protected@file@percent }
\newlabel{sec:Franke}{{1.2}{2}{Franke function}{subsection.1.2}{}}
\newlabel{sec:Franke@cref}{{[subsection][2][1]1.2}{[1][2][]2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}MNIST dataset}{2}{subsection.1.3}\protected@file@percent }
\newlabel{sec:MNIST}{{1.3}{2}{MNIST dataset}{subsection.1.3}{}}
\newlabel{sec:MNIST@cref}{{[subsection][3][1]1.3}{[1][2][]2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Ordinary Least Squares (OLS) and Ridge}{2}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Stochastic Gradient Descent (SGD)}{3}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}Momentum}{3}{subsubsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.2}Minibatching}{3}{subsubsection.1.5.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.3}Varying step length}{3}{subsubsection.1.5.3}\protected@file@percent }
\newlabel{sec:varying step length}{{1.5.3}{3}{Varying step length}{subsubsection.1.5.3}{}}
\newlabel{sec:varying step length@cref}{{[subsubsection][3][1,5]1.5.3}{[1][3][]3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Logistic Regression}{4}{subsection.1.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.6.1}$l_2$ regularization parameter}{4}{subsubsection.1.6.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Neural Network}{4}{subsection.1.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}SGD}{4}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Logistic Regression}{5}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Neural Network}{5}{subsection.2.3}\protected@file@percent }
\newlabel{sec:NN method}{{2.3}{5}{Neural Network}{subsection.2.3}{}}
\newlabel{sec:NN method@cref}{{[subsection][3][2]2.3}{[1][5][]5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Results/Discussion}{7}{section.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}SGD}{7}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Shows how well the SGD solution approaches the Frankefunction as a function of the number of epochs by using the MSE of the training data.\relax }}{7}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:partA_MSE}{{1}{7}{Shows how well the SGD solution approaches the Frankefunction as a function of the number of epochs by using the MSE of the training data.\relax }{figure.caption.1}{}}
\newlabel{fig:partA_MSE@cref}{{[figure][1][]1}{[1][7][]7}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Shows how well the SGD solution approaches the Frankefunction as a function of the number of epochs by using the $R^2$-score of the training data.\relax }}{8}{figure.caption.2}\protected@file@percent }
\newlabel{fig:partA_R2}{{2}{8}{Shows how well the SGD solution approaches the Frankefunction as a function of the number of epochs by using the $R^2$-score of the training data.\relax }{figure.caption.2}{}}
\newlabel{fig:partA_R2@cref}{{[figure][2][]2}{[1][7][]8}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Shows how well the ridge solution approaches the Frankefunction as a function of the $\lambda $-parameter by using the $R^2$-score.\relax }}{8}{figure.caption.3}\protected@file@percent }
\newlabel{fig:partA_ridge}{{3}{8}{Shows how well the ridge solution approaches the Frankefunction as a function of the $\lambda $-parameter by using the $R^2$-score.\relax }{figure.caption.3}{}}
\newlabel{fig:partA_ridge@cref}{{[figure][3][]3}{[1][8][]8}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces A sample of $\lambda $-values with an associated $R^2$ score.\relax }}{9}{table.caption.4}\protected@file@percent }
\newlabel{tab:Ridge performance}{{1}{9}{A sample of $\lambda $-values with an associated $R^2$ score.\relax }{table.caption.4}{}}
\newlabel{tab:Ridge performance@cref}{{[table][1][]1}{[1][9][]9}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Shows score reached as a function of $\eta $. An $\eta $-value containing a "None" means that the other value is considered as a constant eta value.\relax }}{9}{table.caption.5}\protected@file@percent }
\newlabel{tab:SGD eta-values}{{2}{9}{Shows score reached as a function of $\eta $. An $\eta $-value containing a "None" means that the other value is considered as a constant eta value.\relax }{table.caption.5}{}}
\newlabel{tab:SGD eta-values@cref}{{[table][2][]2}{[1][9][]9}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Optimal values found with associated $r^2$ score. Franke was generated with 200 points on each axis, and a design matrix of polynomial degree 15 was used.\relax }}{9}{table.caption.6}\protected@file@percent }
\newlabel{tab:SGD optimal values}{{3}{9}{Optimal values found with associated $r^2$ score. Franke was generated with 200 points on each axis, and a design matrix of polynomial degree 15 was used.\relax }{table.caption.6}{}}
\newlabel{tab:SGD optimal values@cref}{{[table][3][]3}{[1][9][]9}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Logistic Regression}{10}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces shows how well the logistic regression predicts the correct labels as a function of the $l_2$- and $\eta $-parameter. The $\eta $-reference index refers to $t_0 \text  { and } t_1$ in order as (1.0, 100.0), (1.0, 31.622777), (1.0, 10.0), (1.0, 3.162278), (1.0, 1.0).\relax }}{10}{figure.caption.7}\protected@file@percent }
\newlabel{fig:partE_l2andeta}{{4}{10}{shows how well the logistic regression predicts the correct labels as a function of the $l_2$- and $\eta $-parameter. The $\eta $-reference index refers to $t_0 \text { and } t_1$ in order as (1.0, 100.0), (1.0, 31.622777), (1.0, 10.0), (1.0, 3.162278), (1.0, 1.0).\relax }{figure.caption.7}{}}
\newlabel{fig:partE_l2andeta@cref}{{[figure][4][]4}{[1][9][]10}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Optimal values found with associated accuracy score.\relax }}{10}{table.caption.8}\protected@file@percent }
\newlabel{tab:LogReg optimal values}{{4}{10}{Optimal values found with associated accuracy score.\relax }{table.caption.8}{}}
\newlabel{tab:LogReg optimal values@cref}{{[table][4][]4}{[1][10][]10}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Neural Network}{10}{subsection.3.3}\protected@file@percent }
\newlabel{sec:NN results}{{3.3}{10}{Neural Network}{subsection.3.3}{}}
\newlabel{sec:NN results@cref}{{[subsection][3][3]3.3}{[1][10][]10}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces $R^2$ score reached for each activation function\relax }}{10}{table.caption.9}\protected@file@percent }
\newlabel{tab:activation functions}{{5}{10}{$R^2$ score reached for each activation function\relax }{table.caption.9}{}}
\newlabel{tab:activation functions@cref}{{[table][5][]5}{[1][10][]10}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces $R^2$ score as a function of $\eta $ and $\lambda $. The axes are numbered by indexes, not actual axis value, but axis labels describe the conversion between the values and the indexes. The $\eta $-axis is split into constant $\eta $, indices 0 to 6 inclusive, and varying $\eta $, indices 7 to 13 inclusive. Varying $\eta $ assumes $\eta = t_0/(t+t_1)$ where $t_0=1$\relax }}{11}{figure.caption.10}\protected@file@percent }
\newlabel{fig:covers the page}{{5}{11}{$R^2$ score as a function of $\eta $ and $\lambda $. The axes are numbered by indexes, not actual axis value, but axis labels describe the conversion between the values and the indexes. The $\eta $-axis is split into constant $\eta $, indices 0 to 6 inclusive, and varying $\eta $, indices 7 to 13 inclusive. Varying $\eta $ assumes $\eta = t_0/(t+t_1)$ where $t_0=1$\relax }{figure.caption.10}{}}
\newlabel{fig:covers the page@cref}{{[figure][5][]5}{[1][10][]11}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Optimal parameters with an increased number of epochs. The classification network was trained for 1000 epochs with a minibatch of 32 datapoints, while the linear regression network was trained for 5000 epochs with a minibatch of 32 datapoints.\relax }}{12}{table.caption.11}\protected@file@percent }
\newlabel{tab:optimal NN}{{6}{12}{Optimal parameters with an increased number of epochs. The classification network was trained for 1000 epochs with a minibatch of 32 datapoints, while the linear regression network was trained for 5000 epochs with a minibatch of 32 datapoints.\relax }{table.caption.11}{}}
\newlabel{tab:optimal NN@cref}{{[table][6][]6}{[1][12][]12}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Comparing the algorithms}{12}{subsection.3.4}\protected@file@percent }
\newlabel{sec:algorithm-comparison}{{3.4}{12}{Comparing the algorithms}{subsection.3.4}{}}
\newlabel{sec:algorithm-comparison@cref}{{[subsection][4][3]3.4}{[1][12][]12}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Regression}{12}{subsubsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Classification}{12}{subsubsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Improvements}{12}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{13}{section.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}References}{13}{section.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6}Appendix}{13}{section.6}\protected@file@percent }
